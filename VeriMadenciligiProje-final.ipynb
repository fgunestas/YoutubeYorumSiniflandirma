{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-96-90166ccd5c56>, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-96-90166ccd5c56>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    
      LOPER_KEY = #\"AIzaSyDhOuMRp8V1x2ktKZifK6v8Q6gLYiervIQ\"\u001b[0m\n\u001b[0m                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Sample Python code for youtube.commentThreads.list\n",
    "# See instructions for running these code samples locally:\n",
    "# https://developers.google.com/explorer-help/guides/code_samples#python\n",
    "\n",
    "import os\n",
    "import googleapiclient.discovery\n",
    "\n",
    "def getComments():\n",
    "    # Disable OAuthlib's HTTPS verification when running locally.\n",
    "    # *DO NOT* leave this option enabled in production.\n",
    "    os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
    "\n",
    "    api_service_name = \"youtube\"\n",
    "    api_version = \"v3\"\n",
    "    DEVELOPER_KEY = #\"\"\n",
    "\n",
    "    youtube = googleapiclient.discovery.build(\n",
    "        api_service_name, api_version, developerKey = DEVELOPER_KEY)\n",
    "\n",
    "    request = youtube.commentThreads().list(\n",
    "        part=\"snippet,replies\",\n",
    "        videoId=\"eVD9j36Ke94\",\n",
    "        order=\"relevance\",\n",
    "        maxResults=100\n",
    "        \n",
    "    )\n",
    "    response = request.execute()\n",
    "    \n",
    "    import csv\n",
    "    with open('dataset.csv', 'a', newline='',encoding='utf_8') as csvfile:\n",
    "        comment_writer = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        for i in range(100):\n",
    "            comment_writer.writerow([response[\"items\"][i][\"snippet\"]['topLevelComment'][\"snippet\"]['textOriginal']])\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    getComments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veri Temizleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Yorumlar</th>\n",
       "      <th>Etiket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This song got over a billion views for a reaso...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>People who dislike this song arenâ€™t human</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This song gives feelings that never happened.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i usually don't like rock song but this one hi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How come someone dislike song like this.i love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>get ready for this comment\\n\\n\\n\\nwho's listen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Few days 2019 will end but still I loved this ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>I don't understand why would people dislike th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Who else is just randomly listening to some ol...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>who's hearing this amazing song in 2019? :D</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>People who disliked this song don't have ears.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>I hate when talented people suicide.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1 billion. we made chester proud.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Whatever you died our heart your still listeni...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>why are their so many new comments recently? b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>On reflection Chester was singing about his ow...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Let's get this video to 1 Billion views in hon...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>I didn't have a laptop, so was going to the Cy...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>*Don't be sad that he's gone, Be happy because...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>If youâ€™re listening to this in 2019... i lik...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Let's make this the most viewed rock song on Y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>BACK TO THE DAYS WHEN HEAVY METALLIC MUSIC WAS...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>*0% Porn*\\n*100% Music*\\n*R.I.P Chester*</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Doctor: you have 2 minutes live\\nMe: *star lin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>how is it that a song summarizes my life :(</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>I'm gonna tell my kids this was Linkin Park.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>R.I.P chester. i feel like the world has almos...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>The energy of Linkin Park is just unique</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Few more hours til Chester's 2nd death anniver...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>december 2019 still missing Chester,still list...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3782</th>\n",
       "      <td>Never rely on edited comments.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3801</th>\n",
       "      <td>Im a zero. This comment only gets likes from o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3822</th>\n",
       "      <td>After this song zero feels like a compliment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3828</th>\n",
       "      <td>It's referring to the rating system where peop...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3836</th>\n",
       "      <td>Shit my teacher played this song in class so m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3842</th>\n",
       "      <td>Wirehead firezoid orpheus godsolo and orphens ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3848</th>\n",
       "      <td>Spelling has left the chat</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3852</th>\n",
       "      <td>i am so glad zero is at least appreciated by s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3858</th>\n",
       "      <td>Ralph wrecks the internet brought me here.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3863</th>\n",
       "      <td>It's supposed to be light, but it makes me cry</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3961</th>\n",
       "      <td>I think about this song whenever I see my ex-g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4002</th>\n",
       "      <td>When I was younger this was my shit bro and no...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4011</th>\n",
       "      <td>If you remember this song, sorry youâ€™re old</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4259</th>\n",
       "      <td>Yep he definitely raps like a robot maybe he i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4362</th>\n",
       "      <td>All the dislikes are cops</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4365</th>\n",
       "      <td>Reply to this comment in Russian if you're gay</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5038</th>\n",
       "      <td>This song should make you cry and want to drin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5090</th>\n",
       "      <td>This song makes me want to sleep with one foot...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5093</th>\n",
       "      <td>This song makes me wanna do a drive-by with a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5095</th>\n",
       "      <td>This song smacks it makes me want to steal eve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5097</th>\n",
       "      <td>This song makes me want to uncook a cooked burger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5099</th>\n",
       "      <td>This song makes me wanna start swimming in a p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5950</th>\n",
       "      <td>The dressing is not like Indian wear for ladie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5959</th>\n",
       "      <td>The relationship between INDIA and PAKISTAN is...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6005</th>\n",
       "      <td>she doesn't take drugs. she is drugs.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6007</th>\n",
       "      <td>Don't believe I still like this song after 4 y...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6008</th>\n",
       "      <td>I don't care if you're watching in 2019</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6028</th>\n",
       "      <td>Wtf, im searching for kamikaze pilot and now i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6029</th>\n",
       "      <td>looks like russian party</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6054</th>\n",
       "      <td>I remember when i listened to this on the radi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>622 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Yorumlar Etiket\n",
       "0     This song got over a billion views for a reaso...      1\n",
       "1           People who dislike this song arenâ€™t human      1\n",
       "3         This song gives feelings that never happened.      1\n",
       "6     i usually don't like rock song but this one hi...      1\n",
       "9     How come someone dislike song like this.i love...      1\n",
       "11    get ready for this comment\\n\\n\\n\\nwho's listen...      1\n",
       "16    Few days 2019 will end but still I loved this ...      1\n",
       "18    I don't understand why would people dislike th...      1\n",
       "19    Who else is just randomly listening to some ol...      1\n",
       "20          who's hearing this amazing song in 2019? :D      1\n",
       "22       People who disliked this song don't have ears.      1\n",
       "23                 I hate when talented people suicide.      1\n",
       "24                    1 billion. we made chester proud.      1\n",
       "26    Whatever you died our heart your still listeni...      1\n",
       "27    why are their so many new comments recently? b...      1\n",
       "28    On reflection Chester was singing about his ow...      1\n",
       "29    Let's get this video to 1 Billion views in hon...      1\n",
       "30    I didn't have a laptop, so was going to the Cy...      1\n",
       "34    *Don't be sad that he's gone, Be happy because...      1\n",
       "37    If youâ€™re listening to this in 2019... i lik...      1\n",
       "39    Let's make this the most viewed rock song on Y...      1\n",
       "43    BACK TO THE DAYS WHEN HEAVY METALLIC MUSIC WAS...      1\n",
       "44             *0% Porn*\\n*100% Music*\\n*R.I.P Chester*      1\n",
       "46    Doctor: you have 2 minutes live\\nMe: *star lin...      1\n",
       "47          how is it that a song summarizes my life :(      1\n",
       "55         I'm gonna tell my kids this was Linkin Park.      1\n",
       "57    R.I.P chester. i feel like the world has almos...      1\n",
       "58             The energy of Linkin Park is just unique      1\n",
       "59    Few more hours til Chester's 2nd death anniver...      1\n",
       "60    december 2019 still missing Chester,still list...      1\n",
       "...                                                 ...    ...\n",
       "3782                     Never rely on edited comments.      0\n",
       "3801  Im a zero. This comment only gets likes from o...      0\n",
       "3822       After this song zero feels like a compliment      0\n",
       "3828  It's referring to the rating system where peop...      0\n",
       "3836  Shit my teacher played this song in class so m...      0\n",
       "3842  Wirehead firezoid orpheus godsolo and orphens ...      0\n",
       "3848                         Spelling has left the chat      0\n",
       "3852  i am so glad zero is at least appreciated by s...      0\n",
       "3858         Ralph wrecks the internet brought me here.      0\n",
       "3863     It's supposed to be light, but it makes me cry      0\n",
       "3961  I think about this song whenever I see my ex-g...      0\n",
       "4002  When I was younger this was my shit bro and no...      0\n",
       "4011      If you remember this song, sorry youâ€™re old      0\n",
       "4259  Yep he definitely raps like a robot maybe he i...      0\n",
       "4362                          All the dislikes are cops      0\n",
       "4365     Reply to this comment in Russian if you're gay      0\n",
       "5038  This song should make you cry and want to drin...      0\n",
       "5090  This song makes me want to sleep with one foot...      0\n",
       "5093  This song makes me wanna do a drive-by with a ...      0\n",
       "5095  This song smacks it makes me want to steal eve...      0\n",
       "5097  This song makes me want to uncook a cooked burger      0\n",
       "5099  This song makes me wanna start swimming in a p...      0\n",
       "5950  The dressing is not like Indian wear for ladie...      0\n",
       "5959  The relationship between INDIA and PAKISTAN is...      0\n",
       "6005              she doesn't take drugs. she is drugs.      0\n",
       "6007  Don't believe I still like this song after 4 y...      0\n",
       "6008            I don't care if you're watching in 2019      0\n",
       "6028  Wtf, im searching for kamikaze pilot and now i...      0\n",
       "6029                           looks like russian party      0\n",
       "6054  I remember when i listened to this on the radi...      0\n",
       "\n",
       "[622 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import nltk,csv,numpy \n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "import string\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "df = pd.read_excel('dataset.xlsx')\n",
    "df1=pd.DataFrame(df[df.Etiket==1])\n",
    "df0=pd.DataFrame(df[df.Etiket==0])\n",
    "df2=df1.append(df0)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6100"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=df2['Yorumlar']\n",
    "y=df2['Etiket']\n",
    "y\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    try:\n",
    "        if y[i]==1:\n",
    "        \n",
    "            y[i]='1'\n",
    "        else:\n",
    "        \n",
    "            y[i]='0'\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "txt=X[1]\n",
    "txt=txt.replace(\"â€™\",\"'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['People', 'who', 'dislike', 'this', 'song', 'aren', 't', 'human']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "t_word=tokenizer.tokenize(txt)\n",
    "t_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"People who dislike this song aren't human\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "tokenized_text=sent_tokenize(txt)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['People', 'who', 'dislike', 'this', 'song', 'are', \"n't\", 'human']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_word=word_tokenize(txt)\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'off', 'again', 'from', 'through', 't', 'she', 'between', 'yours', 'ma', 'couldn', 'whom', 'the', 'mightn', 'does', \"needn't\", 'once', 'has', 'this', 'yourselves', 'my', 'who', 'can', 'here', 'wouldn', 's', 'i', 'own', 'll', 'all', 'or', \"you've\", 'very', 'will', 'both', 'while', 'doesn', \"won't\", 'being', 'than', \"you're\", 'am', 'to', 'in', 'nor', 'is', 'but', 'on', 'weren', 'myself', 'should', 'out', 'more', 'because', \"it's\", 'his', 'with', 'any', 'not', 'shouldn', \"you'd\", 'these', 'against', 'only', \"haven't\", 'where', 'ourselves', 'under', 'then', 'having', 'how', 'most', 'hers', 'as', 'and', \"couldn't\", 'such', \"shouldn't\", \"isn't\", 'by', 'its', 'her', 'herself', 'their', 'same', \"hadn't\", 'he', 'a', 'needn', 'don', \"hasn't\", 'those', 'too', 'been', \"mustn't\", 'didn', 'some', 'o', 'what', 'just', 'theirs', 'himself', 've', \"should've\", \"doesn't\", 'have', 'isn', \"that'll\", 'of', 'before', \"wouldn't\", \"she's\", 'themselves', 'during', 'do', 'we', 'him', 'hasn', 'ours', \"weren't\", 'me', 'be', 'your', 'up', 'further', 'over', 'you', 'had', 'won', 'that', 'hadn', \"don't\", 'itself', 'them', 'doing', 'when', 'wasn', 'if', 'it', 'y', 'shan', 'down', 'below', 'are', 'no', 'they', 'after', 'our', 'was', 'd', \"mightn't\", \"didn't\", 'which', 're', 'm', 'into', 'about', 'for', 'now', 'aren', \"shan't\", 'did', 'were', 'until', 'there', 'why', 'mustn', 'few', 'other', \"wasn't\", 'at', 'each', 'yourself', 'so', 'an', \"you'll\", 'haven', 'ain', 'above', \"aren't\"}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filterd Sentence: ['People', 'dislike', 'song', 'human']\n"
     ]
    }
   ],
   "source": [
    "filtered_sent=[]\n",
    "for w in t_word:\n",
    "    if w not in stop_words:\n",
    "        filtered_sent.append(w)\n",
    "        #print(\"Tokenized Sentence:\",tokenized_text)\n",
    "print(\"Filterd Sentence:\",filtered_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\1700003918\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Word: People\n",
      "Stemmed Word: peopl\n",
      "Lemmatized Word: dislike\n",
      "Stemmed Word: dislik\n",
      "Lemmatized Word: song\n",
      "Stemmed Word: song\n",
      "Lemmatized Word: human\n",
      "Stemmed Word: human\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stem = PorterStemmer()\n",
    "\n",
    "for i in range(len(filtered_sent)):\n",
    "    word = filtered_sent[i]\n",
    "    print(\"Lemmatized Word:\",lem.lemmatize(word))\n",
    "    print(\"Stemmed Word:\",stem.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\1700003918\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('People', 'NNS'),\n",
       " ('who', 'WP'),\n",
       " ('dislike', 'VBP'),\n",
       " ('this', 'DT'),\n",
       " ('song', 'NN'),\n",
       " ('aren', 'VBZ'),\n",
       " ('t', 'JJ'),\n",
       " ('human', 'JJ')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.pos_tag(t_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#tokenizer to remove unwanted elements from out data like symbols and numbers\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1),\n",
    "                     tokenizer = token.tokenize)\n",
    "text_counts= cv.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_counts, y, test_size=0.3, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "lr_predicted = lr.predict(X_test)\n",
    "confusion = confusion_matrix(y_test, lr_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 skoru : 0.9275362318840579\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 skoru :\",f1_score(lr_predicted,y_test,pos_label='1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doğruluk : 0.8663101604278075\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(\"Doğruluk :\",metrics.accuracy_score(y_test,lr_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duyarlılık : 0.9876543209876543\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "print(\"Duyarlılık :\",recall_score(y_test,lr_predicted,pos_label='1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keskinlik : 0.8743169398907104\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "print(\"Keskinlik :\",precision_score(y_test,lr_predicted,pos_label='1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
